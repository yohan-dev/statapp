{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>scores</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US-British actress Sienna Miller poses after u...</td>\n",
       "      <td>{'US': 0.978723404, 'British': 0.978723404, 'S...</td>\n",
       "      <td>{'France': 0.16666666666666666, 'British': 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People gather at a shopping mall in the Shatin...</td>\n",
       "      <td>{'Shatin': 0.8297872340000001, 'Hong Kong': 1....</td>\n",
       "      <td>{'Hong Kong': 0.5, 'Shatin': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFP presents a photo essay of 19 images by pho...</td>\n",
       "      <td>{'AFP': 1.0, 'Christof Stache': 0.829787234000...</td>\n",
       "      <td>{'AFP': 0.25, 'Slug GERMANY-AGRICULTURE-CATTLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Palestinian worker harvests dates in the Jor...</td>\n",
       "      <td>{'Palestinian': 0.489361702, 'Jordan Valley': ...</td>\n",
       "      <td>{'West Bank': 0.07142857142857142, \"the West B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Turkish Trade Minister Ruhsar Pekcan (R) and U...</td>\n",
       "      <td>{'Turkish': 0.489361702, 'Ruhsar Pekcan': 0.93...</td>\n",
       "      <td>{'Commerce': 0.16666666666666666, 'Ruhsar Pekc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  US-British actress Sienna Miller poses after u...   \n",
       "1  People gather at a shopping mall in the Shatin...   \n",
       "2  AFP presents a photo essay of 19 images by pho...   \n",
       "3  A Palestinian worker harvests dates in the Jor...   \n",
       "4  Turkish Trade Minister Ruhsar Pekcan (R) and U...   \n",
       "\n",
       "                                              scores  \\\n",
       "0  {'US': 0.978723404, 'British': 0.978723404, 'S...   \n",
       "1  {'Shatin': 0.8297872340000001, 'Hong Kong': 1....   \n",
       "2  {'AFP': 1.0, 'Christof Stache': 0.829787234000...   \n",
       "3  {'Palestinian': 0.489361702, 'Jordan Valley': ...   \n",
       "4  {'Turkish': 0.489361702, 'Ruhsar Pekcan': 0.93...   \n",
       "\n",
       "                                                freq  \n",
       "0  {'France': 0.16666666666666666, 'British': 0.1...  \n",
       "1                  {'Hong Kong': 0.5, 'Shatin': 0.5}  \n",
       "2  {'AFP': 0.25, 'Slug GERMANY-AGRICULTURE-CATTLE...  \n",
       "3  {'West Bank': 0.07142857142857142, \"the West B...  \n",
       "4  {'Commerce': 0.16666666666666666, 'Ruhsar Pekc...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neuralcoref\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "df1=pd.read_csv(\"scores1_freq.csv\")\n",
    "df2=pd.read_csv(\"scores2_freq.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_scores1=[]\n",
    "for string in df1.scores:\n",
    "    liste_scores1.append(ast.literal_eval(string))   \n",
    "\n",
    "liste_scores2=[]\n",
    "for string in df2.scores:\n",
    "    liste_scores2.append(ast.literal_eval(string))    \n",
    "\n",
    "liste_freq=[]\n",
    "for string in df2.freq:\n",
    "    liste_freq.append(ast.literal_eval(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On importe le fichier contenant les scores de chaque fonction grammaticale\n",
    "\n",
    "scoring = pd.read_csv('scoring.csv', delimiter = \";\")\n",
    "dict_val = {}\n",
    "\n",
    "for i in range(48):\n",
    "    dict_val[scoring['function'][i]] = scoring['score_norm'][i]\n",
    "\n",
    "#On importe un modèle md pour avoir des mots vectorisés\n",
    "nlp=spacy.load(\"en_core_web_md\")  \n",
    "neuralcoref.add_to_pipe(nlp,greedyness=0.5)\n",
    "\n",
    "def dep_ent(ent, doc):\n",
    "    \"\"\" Retourne la fonction grammaticale :  la 'dep', d'une entité. Cette fonction est nécessaire car elle permet d'affecter\n",
    "    une dep à une entité composée de plusieurs mots ayant chacun une dep de base.\n",
    "    Traite aussi le cas particulier des mots étant des conj ou des compound : leur vrai dep et celle du mot auxquels\n",
    "    ils sont associés en tant que conj ou compound.\"\"\"\n",
    "    start= ent.start\n",
    "    end=ent.end\n",
    "    for k in range(start,end):\n",
    "        if doc[k].head.text not in ent.text: \n",
    "            if doc[k].dep_=='conj':     \n",
    "                tok=doc[k]            \n",
    "                while tok.dep_=='conj':\n",
    "                    tok=tok.head      \n",
    "                return(tok.dep_)\n",
    "            \n",
    "            if doc[k].dep_=='compound':   \n",
    "                tok=doc[k]            \n",
    "                while tok.dep_=='compound':\n",
    "                    tok=tok.head      \n",
    "                return(tok.dep_)\n",
    "            return(doc[k].dep_)    \n",
    "    return doc[start].dep_\n",
    "\n",
    "def ent_good_type(ent): #filtre les entités selon leur type\n",
    "    return (ent.label_ == \"PERSON\"or ent.label_ == \"NORP\" or ent.label_ == \"ORG\" or ent.label_ == \"GPE\" or ent.label_ == \"EVENT\" or ent.label_ == \"LOC\")\n",
    "\n",
    "def sort_ent(doc):\n",
    "    \"\"\"Retourne la liste des entités en les filtrant selon leur type et en les triant de manière à avoir au début de\n",
    "    la liste les entités ayant des coréférences.\"\"\"\n",
    "    ent_coref=[ent for ent in doc.ents if ent._.is_coref and ent_good_type(ent)]\n",
    "    ent_vanilla=[ent for ent in doc.ents if ent_good_type(ent) and not ent._.is_coref]\n",
    "    return ent_coref + ent_vanilla\n",
    "\n",
    "def scores_doc(doc):\n",
    "    \"\"\"Retourne le score de chaque entité pour la méthode sans neuralcoref.\"\"\"\n",
    "    res={}\n",
    "    for ent in doc.ents:\n",
    "        if ent.text not in res.keys():\n",
    "            res[ent.text]=dict_val[dep_ent(ent,doc)]\n",
    "        else:\n",
    "            res[ent.text]+=dict_val[dep_ent(ent,doc)]\n",
    "    return res\n",
    "\n",
    "def scores_doc_coref1(doc):\n",
    "    \"\"\"Retourne le score de chaque entité pour la méthode de base opérée sur le document resolved.\n",
    "    Le document resolved est le document de base dans lequel toutes les références à un groupe de mot sont remplacées\n",
    "    par celui-ci.\n",
    "    Par exemple : My dad is home. He watches TV devient My dad is home. My dad watches TV.\n",
    "    En raisonnant avec le nlp sur le document resolved, le nlp va détecter beaucoup plus de fois la même entité.\n",
    "    Le principal inconvénient est que toutes les références sont remplacées, y compris celles qui ne sont pas associées \n",
    "    à des entités mais plutôt à des très longs bouts de phrase qui sont repris par un pronom comme \"it\"  \"\"\"\n",
    "    \n",
    "    doc=nlp(doc._.coref_resolved)\n",
    "    res={}\n",
    "    for ent in doc.ents:\n",
    "        if ent_good_type(ent):\n",
    "            if ent.text not in res.keys():\n",
    "                res[ent.text]=dict_val[dep_ent(ent,doc)]\n",
    "            else:\n",
    "                res[ent.text]+=dict_val[dep_ent(ent,doc)]\n",
    "    return res\n",
    "\n",
    "def is_in_cluster(ent,cluster):  #détermine si une entité est dans une des mentions d'un cluster\n",
    "    for span in cluster.mentions:\n",
    "        if ent.text in span.text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def scores_doc_coref2(doc):\n",
    "    \"\"\"Cette méthode utilise neuralcoref mais au lieu d'agir sur le doc resolved, on va chercher a l'interieur des clusters\n",
    "    associés au document. Pour rappel, un cluster contient un mot et ses références. Ce mot n'est pas forcément une entité.\n",
    "    Un cluster est de la forme Trump : [Trump, he].\n",
    "    Pour chaque entité, on va d'abord regarder si elle est coréférencée (donc mentionnée explicitement dans un cluster)\n",
    "    ou pas.\n",
    "    Si elle est coréférencée, on va chercher le cluster qui lui est associé.\n",
    "    Une fois dans ce cluster, on va regarder pour chaque mention (réf à l'entité) si elle contient ou pas des entités.\n",
    "    Si c'est le cas, on va alors indiquer que cette entité ne doit pas être retraitée par la suite car elle est traitée en\n",
    "    tant qu'autre mention de l'entité que l'on traite actuellement.\n",
    "    Par exemple, pour l'entité référencée Trump, on trouve le cluster suivant : Trump : [Trump, Donald Trump, he].\n",
    "    Alors l'entité Donald Trump va être marquée comme traitée car elle est en fait équivalente à Trump.\n",
    "    \"he\" n'étant pas une entité on a pas ce pb pour cette mention la.\n",
    "    Ensuite on calcule le score de chaque mention (Trump, Donald Trump et he) en se basant sur leur fonction grammaticale.\n",
    "    Puis on ajoute ces scores à celui de l'entité de base (Trump).\n",
    "    \n",
    "    Si l'entité de base n'est pas référencée, c'est un peu le même principe, la seule complexité en plus étant le fait que\n",
    "    l'entité n'est pas forcément mentionnée telle quelle dans les mentions d'un cluster. \n",
    "    Par exemple, si notre entité de base est Sienna Miller, il est possible qu'on ait un cluster de la forme\n",
    "    The british actress Sienna Miller : [The british actress Sienna Miller, she].\n",
    "    Dans ce cas il faut rechercher si chaque mention contient l'entité de base avant de procéder comme pour les \n",
    "    entités référencées.\n",
    "    \n",
    "    \"\"\"\n",
    "    clusters=doc._.coref_clusters  #liste des clusters du doc. \n",
    "    res={} #le resultat sera un dictionnaire qui permet d'associer un score à chque entité\n",
    "    ent_treated={} #dictionnaire pour différencier les entités traitées des autres\n",
    "    ents_sorted=sort_ent(doc)  #on place les entités ayant une coref avant les autres\n",
    "    for ent in ents_sorted:   \n",
    "        ent_treated[ent.text]=0  #on initialise en affectant 0 à toutes les entités\n",
    "\n",
    "    for ent in ents_sorted:     #pour chaque entité\n",
    "        if ent_treated[ent.text]==0: #si elle n'est pas considérée comme \"traitée\"\n",
    "            if ent._.is_coref:  #si elle est coréférencée (donc si elle apparait en tant que mention pour un cluster )\n",
    "                #print(ent)\n",
    "                for cluster in clusters:  #on va chercher le cluster associé à l'entité puisqu'elle est coref\n",
    "                    if ent in cluster.mentions: # on regarde si l'entité est dans les mentions du cluster\n",
    "                        for span in cluster.mentions: #Mtnt qu'on est dans le bon cluster, on regarde pour chaque span\n",
    "                            if span.ents != [] : \n",
    "                                if ent.has_vector:\n",
    "                                    max_sim=0\n",
    "                                    max_span_ent=span.ents[0]\n",
    "                                    for span_ent in span.ents: #on cherche l'entité dans le span la plus proche de ent\n",
    "                                        if span_ent.has_vector:\n",
    "                                            if ent.similarity(span_ent) > max_sim:\n",
    "                                                max_span_ent=span_ent\n",
    "                                                max_sim=ent.similarity(span_ent)\n",
    "                                    ent_treated[max_span_ent.text]=1 #une fois qu'on l'a trouvé on la marque comme traitée\n",
    "                            else:\n",
    "                                max_span_ent=span        \n",
    "                            if ent.text not in res.keys(): \n",
    "                                res[ent.text]=dict_val[dep_ent(max_span_ent,doc)]  #on affecte le score de l'entité\n",
    "                            else:\n",
    "                                res[ent.text] += dict_val[dep_ent(max_span_ent,doc)] \n",
    "            else: #si l'entité n'est pas exactement coréférencée par neuralcoref\n",
    "                flag=0\n",
    "                for cluster in clusters:\n",
    "                    if is_in_cluster(ent,cluster) and ent.label_ != 'NORP': \n",
    "                        flag=1\n",
    "                        for span in cluster.mentions:\n",
    "                            #print(span)\n",
    "                            if span.ents != [] :\n",
    "                                if ent.has_vector:\n",
    "                                    max_sim=0\n",
    "                                    max_span_ent=span.ents[0]\n",
    "                                    for span_ent in span.ents:\n",
    "                                        if span_ent.has_vector:\n",
    "                                            if ent.similarity(span_ent) > max_sim:\n",
    "                                                max_span_ent=span_ent\n",
    "                                                max_sim=ent.similarity(span_ent)\n",
    "                                    ent_treated[max_span_ent.text]=1\n",
    "                            else:\n",
    "                                max_span_ent=span #on peut peut etre supprimer cette branche else et...\n",
    "                            if ent.text not in res.keys():\n",
    "                                res[ent.text]=dict_val[dep_ent(max_span_ent,doc)] #...replacer max_span_ent par span ici\n",
    "                            else:\n",
    "                                res[ent.text] += dict_val[dep_ent(max_span_ent,doc)]  #et ici\n",
    "                if flag==0: #si l'entité n'est vraiment dans aucun cluster   \n",
    "                    if ent.text not in res.keys():\n",
    "                        res[ent.text]=dict_val[dep_ent(ent,doc)]\n",
    "                    else:\n",
    "                        res[ent.text]+=dict_val[dep_ent(ent,doc)] \n",
    "    return res\n",
    "\n",
    "\n",
    "def freq_dict(L):\n",
    "    n = len(L)\n",
    "    L_unique = list(set(L))\n",
    "    d = {}\n",
    "    for a in L_unique:\n",
    "        for b in L:\n",
    "            if a == b:\n",
    "                if a in d.keys():\n",
    "                    d[a] += 1/n\n",
    "                else:\n",
    "                    d[a] = 1/n\n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sim1(doc,score_doc,k):\n",
    "    res=0\n",
    "    freq_doc=freq_dict([ent.text for ent in doc.ents])\n",
    "    score_image=liste_scores1[k]\n",
    "    freq_image=liste_freq[k]\n",
    "    for i in score_doc.keys():\n",
    "        if i in score_image.keys():\n",
    "            res += (score_doc[i]+score_image[i])/2 #- abs(freq_doc[i]-freq_image[i])/2\n",
    "    return res\n",
    "\n",
    "def score_sim2(doc,score_doc,k):\n",
    "    res=0\n",
    "    freq_doc=freq_dict([ent.text for ent in doc.ents])\n",
    "    score_image=liste_scores2[k]\n",
    "    freq_image=liste_freq[k]\n",
    "    for i in score_doc.keys():\n",
    "        if i in score_image.keys():\n",
    "            res += (score_doc[i]+score_image[i])/2 #- abs(freq_doc[i]-freq_image[i])/2\n",
    "    return res\n",
    "\n",
    "def related_descr(doc):\n",
    "    index_list=[]\n",
    "    for k in range(len(liste_scores1)):\n",
    "        for ent in doc.ents:\n",
    "            if ent.text in liste_scores1[k].keys():\n",
    "                index_list.append(k)\n",
    "                break\n",
    "    return index_list\n",
    "\n",
    "def best_image1(doc):\n",
    "    best_score=0\n",
    "    best_descr=0\n",
    "    score_doc=scores_doc_coref1(doc)\n",
    "    for k in related_descr(doc):\n",
    "        if score_sim1(doc,score_doc,k) > best_score:\n",
    "            best_score=score_sim1(doc,score_doc,k)\n",
    "            best_descr=k\n",
    "    return best_descr #,df.description[best_descr],best_score\n",
    "\n",
    "def best_image2(doc):\n",
    "    score_doc=scores_doc_coref2(doc)\n",
    "    best_score=0\n",
    "    best_descr=0\n",
    "    for k in related_descr(doc):\n",
    "        if score_sim2(doc,score_doc,k) > best_score:\n",
    "            best_score=score_sim2(doc,score_doc,k)\n",
    "            best_descr=k\n",
    "    return best_descr #,df.description[best_descr],best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "Newly-initiated 'Naga Sadhus' (Hindu holy men) sit as they perform rituals on the banks of the Ganges River during the Kumbh Mela festival, in Allahabad on February 1, 2019. During every Kumbh Mela, the diksha, a ritual of initiation by a guru takes place for new members. \n",
      " {\"Naga Sadhus'\": 2.0, 'Hindu': 0.9361702120000001, 'the Ganges River': 0.8297872340000001, 'the Kumbh Mela festival': 0.8297872340000001, 'Allahabad': 0.8297872340000001, 'every Kumbh Mela': 0.8297872340000001} \n",
      " Newly-initiated 'Naga Sadhus' (Hindu holy men) sit in line as they perform rituals on the banks of the Ganges River during the Kumbh Mela festival, in Allahabad on February 1, 2019. During every Kumbh Mela, the diksha, a ritual of initiation by a guru takes place for new members. \n",
      " {\"Naga Sadhus'\": 2.0, 'Hindu': 0.9361702120000001, 'the Ganges River': 0.8297872340000001, 'the Kumbh Mela festival': 0.8297872340000001, 'Allahabad': 0.8297872340000001, 'every Kumbh Mela': 0.8297872340000001}\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(df1.description[78])\n",
    "print(best_image1(doc))\n",
    "print(df.description[78], '\\n',liste_scores1[78],'\\n', df.description[77],'\\n',liste_scores1[77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
