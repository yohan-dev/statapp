{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>scores</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US-British actress Sienna Miller poses after u...</td>\n",
       "      <td>{'US': 0.872340425, 'British': 0.489361702, 'S...</td>\n",
       "      <td>{'France': 0.16666666666666666, 'British': 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People gather at a shopping mall in the Shatin...</td>\n",
       "      <td>{'Hong Kong': 2.489361702, 'Shatin': 0.8297872...</td>\n",
       "      <td>{'Hong Kong': 0.5, 'Shatin': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFP presents a photo essay of 19 images by pho...</td>\n",
       "      <td>{'AFP': 1.0, 'Christof Stache': 0.829787234000...</td>\n",
       "      <td>{'AFP': 0.25, 'Slug GERMANY-AGRICULTURE-CATTLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Palestinian worker harvests dates in the Jor...</td>\n",
       "      <td>{\"the West Bank's\": 1.212765957, 'Netanyahu': ...</td>\n",
       "      <td>{'West Bank': 0.07142857142857142, \"the West B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Turkish Trade Minister Ruhsar Pekcan (R) and U...</td>\n",
       "      <td>{'Turkish': 0.489361702, 'Ruhsar Pekcan': 0.93...</td>\n",
       "      <td>{'Commerce': 0.16666666666666666, 'Ruhsar Pekc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  US-British actress Sienna Miller poses after u...   \n",
       "1  People gather at a shopping mall in the Shatin...   \n",
       "2  AFP presents a photo essay of 19 images by pho...   \n",
       "3  A Palestinian worker harvests dates in the Jor...   \n",
       "4  Turkish Trade Minister Ruhsar Pekcan (R) and U...   \n",
       "\n",
       "                                              scores  \\\n",
       "0  {'US': 0.872340425, 'British': 0.489361702, 'S...   \n",
       "1  {'Hong Kong': 2.489361702, 'Shatin': 0.8297872...   \n",
       "2  {'AFP': 1.0, 'Christof Stache': 0.829787234000...   \n",
       "3  {\"the West Bank's\": 1.212765957, 'Netanyahu': ...   \n",
       "4  {'Turkish': 0.489361702, 'Ruhsar Pekcan': 0.93...   \n",
       "\n",
       "                                                freq  \n",
       "0  {'France': 0.16666666666666666, 'British': 0.1...  \n",
       "1                  {'Hong Kong': 0.5, 'Shatin': 0.5}  \n",
       "2  {'AFP': 0.25, 'Slug GERMANY-AGRICULTURE-CATTLE...  \n",
       "3  {'West Bank': 0.07142857142857142, \"the West B...  \n",
       "4  {'Commerce': 0.16666666666666666, 'Ruhsar Pekc...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neuralcoref\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "df=pd.read_csv(\"scores_freq.csv\")\n",
    "df1=pd.read_csv(\"scores1_freq.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_scores=[]\n",
    "for string in df.scores:\n",
    "    liste_scores.append(ast.literal_eval(string))\n",
    "\n",
    "liste_freq=[]\n",
    "for string in df.freq:\n",
    "    liste_freq.append(ast.literal_eval(string))\n",
    "\n",
    "liste_scores1=[]\n",
    "for string in df1.scores:\n",
    "    liste_scores1.append(ast.literal_eval(string))   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On importe le fichier contenant les scores de chaque fonction grammaticale\n",
    "\n",
    "scoring = pd.read_csv('scoring.csv', delimiter = \";\")\n",
    "dict_val = {}\n",
    "\n",
    "for i in range(48):\n",
    "    dict_val[scoring['function'][i]] = scoring['score_norm'][i]\n",
    "\n",
    "#On importe un modèle md pour avoir des mots vectorisés\n",
    "nlp=spacy.load(\"en_core_web_md\")  \n",
    "neuralcoref.add_to_pipe(nlp,greedyness=0.5)\n",
    "\n",
    "def dep_ent(ent, doc):\n",
    "    \"\"\" Retourne la fonction grammaticale :  la 'dep', d'une entité. Cette fonction est nécessaire car elle permet d'affecter\n",
    "    une dep à une entité composée de plusieurs mots ayant chacun une dep de base.\n",
    "    Traite aussi le cas particulier des mots étant des conj ou des compound : leur vrai dep et celle du mot auxquels\n",
    "    ils sont associés en tant que conj ou compound.\"\"\"\n",
    "    start= ent.start\n",
    "    end=ent.end\n",
    "    for k in range(start,end):\n",
    "        if doc[k].head.text not in ent.text: \n",
    "            if doc[k].dep_=='conj':     \n",
    "                tok=doc[k]            \n",
    "                while tok.dep_=='conj':\n",
    "                    tok=tok.head      \n",
    "                return(tok.dep_)\n",
    "            \n",
    "            if doc[k].dep_=='compound':   \n",
    "                tok=doc[k]            \n",
    "                while tok.dep_=='compound':\n",
    "                    tok=tok.head      \n",
    "                return(tok.dep_)\n",
    "            return(doc[k].dep_)    \n",
    "    return doc[start].dep_\n",
    "\n",
    "def ent_good_type(ent): #filtre les entités selon leur type\n",
    "    return (ent.label_ == \"PERSON\"or ent.label_ == \"NORP\" or ent.label_ == \"ORG\" or ent.label_ == \"GPE\" or ent.label_ == \"EVENT\" or ent.label_ == \"LOC\")\n",
    "\n",
    "def sort_ent(doc):\n",
    "    \"\"\"Retourne la liste des entités en les filtrant selon leur type et en les triant de manière à avoir au début de\n",
    "    la liste les entités ayant des coréférences.\"\"\"\n",
    "    ent_coref=[ent for ent in doc.ents if ent._.is_coref and ent_good_type(ent)]\n",
    "    ent_vanilla=[ent for ent in doc.ents if ent_good_type(ent) and not ent._.is_coref]\n",
    "    return ent_coref + ent_vanilla\n",
    "\n",
    "def scores_doc(doc):\n",
    "    \"\"\"Retourne le score de chaque entité pour la méthode sans neuralcoref.\"\"\"\n",
    "    res={}\n",
    "    for ent in doc.ents:\n",
    "        if ent.text not in res.keys():\n",
    "            res[ent.text]=dict_val[dep_ent(ent,doc)]\n",
    "        else:\n",
    "            res[ent.text]+=dict_val[dep_ent(ent,doc)]\n",
    "    return res\n",
    "\n",
    "def scores_doc_coref1(doc):\n",
    "    \"\"\"Retourne le score de chaque entité pour la méthode de base opérée sur le document resolved.\n",
    "    Le document resolved est le document de base dans lequel toutes les références à un groupe de mot sont remplacées\n",
    "    par celui-ci.\n",
    "    Par exemple : My dad is home. He watches TV devient My dad is home. My dad watches TV.\n",
    "    En raisonnant avec le nlp sur le document resolved, le nlp va détecter beaucoup plus de fois la même entité.\n",
    "    Le principal inconvénient est que toutes les références sont remplacées, y compris celles qui ne sont pas associées \n",
    "    à des entités mais plutôt à des très longs bouts de phrase qui sont repris par un pronom comme \"it\"  \"\"\"\n",
    "    \n",
    "    doc=nlp(doc._.coref_resolved)\n",
    "    res={}\n",
    "    for ent in doc.ents:\n",
    "        if ent.text not in res.keys():\n",
    "            res[ent.text]=dict_val[dep_ent(ent,doc)]\n",
    "        else:\n",
    "            res[ent.text]+=dict_val[dep_ent(ent,doc)]\n",
    "    return res\n",
    "\n",
    "def is_in_cluster(ent,cluster):  #détermine si une entité est dans une des mentions d'un cluster\n",
    "    for span in cluster.mentions:\n",
    "        if ent.text in span.text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def scores_doc_coref2(doc):\n",
    "    \"\"\"Cette méthode utilise neuralcoref mais au lieu d'agir sur le doc resolved, on va chercher a l'interieur des clusters\n",
    "    associés au document. Pour rappel, un cluster contient un mot et ses références. Ce mot n'est pas forcément une entité.\n",
    "    Un cluster est de la forme Trump : [Trump, he].\n",
    "    Pour chaque entité, on va d'abord regarder si elle est coréférencée (donc mentionnée explicitement dans un cluster)\n",
    "    ou pas.\n",
    "    Si elle est coréférencée, on va chercher le cluster qui lui est associé.\n",
    "    Une fois dans ce cluster, on va regarder pour chaque mention (réf à l'entité) si elle contient ou pas des entités.\n",
    "    Si c'est le cas, on va alors indiquer que cette entité ne doit pas être retraitée par la suite car elle est traitée en\n",
    "    tant qu'autre mention de l'entité que l'on traite actuellement.\n",
    "    Par exemple, pour l'entité référencée Trump, on trouve le cluster suivant : Trump : [Trump, Donald Trump, he].\n",
    "    Alors l'entité Donald Trump va être marquée comme traitée car elle est en fait équivalente à Trump.\n",
    "    \"he\" n'étant pas une entité on a pas ce pb pour cette mention la.\n",
    "    Ensuite on calcule le score de chaque mention (Trump, Donald Trump et he) en se basant sur leur fonction grammaticale.\n",
    "    Puis on ajoute ces scores à celui de l'entité de base (Trump).\n",
    "    \n",
    "    Si l'entité de base n'est pas référencée, c'est un peu le même principe, la seule complexité en plus étant le fait que\n",
    "    l'entité n'est pas forcément mentionnée telle quelle dans les mentions d'un cluster. \n",
    "    Par exemple, si notre entité de base est Sienna Miller, il est possible qu'on ait un cluster de la forme\n",
    "    The british actress Sienna Miller : [The british actress Sienna Miller, she].\n",
    "    Dans ce cas il faut rechercher si chaque mention contient l'entité de base avant de procéder comme pour les \n",
    "    entités référencées.\n",
    "    \n",
    "    \"\"\"\n",
    "    clusters=doc._.coref_clusters  #liste des clusters du doc. \n",
    "    res={} #le resultat sera un dictionnaire qui permet d'associer un score à chque entité\n",
    "    ent_treated={} #dictionnaire pour différencier les entités traitées des autres\n",
    "    ents_sorted=sort_ent(doc)  #on place les entités ayant une coref avant les autres\n",
    "    for ent in ents_sorted:   \n",
    "        ent_treated[ent.text]=0  #on initialise en affectant 0 à toutes les entités\n",
    "\n",
    "    for ent in ents_sorted:     #pour chaque entité\n",
    "        if ent_treated[ent.text]==0: #si elle n'est pas considérée comme \"traitée\"\n",
    "            if ent._.is_coref:  #si elle est coréférencée (donc si elle apparait en tant que mention pour un cluster )\n",
    "                #print(ent)\n",
    "                for cluster in clusters:  #on va chercher le cluster associé à l'entité puisqu'elle est coref\n",
    "                    if ent in cluster.mentions: # on regarde si l'entité est dans les mentions du cluster\n",
    "                        for span in cluster.mentions: #Mtnt qu'on est dans le bon cluster, on regarde pour chaque span\n",
    "                            if span.ents != [] : \n",
    "                                if ent.has_vector:\n",
    "                                    max_sim=0\n",
    "                                    max_span_ent=span.ents[0]\n",
    "                                    for span_ent in span.ents: #on cherche l'entité dans le span la plus proche de ent\n",
    "                                        if span_ent.has_vector:\n",
    "                                            if ent.similarity(span_ent) > max_sim:\n",
    "                                                max_span_ent=span_ent\n",
    "                                                max_sim=ent.similarity(span_ent)\n",
    "                                    ent_treated[max_span_ent.text]=1 #une fois qu'on l'a trouvé on la marque comme traitée\n",
    "                            else:\n",
    "                                max_span_ent=span        \n",
    "                            if ent.text not in res.keys(): \n",
    "                                res[ent.text]=dict_val[dep_ent(max_span_ent,doc)]  #on affecte le score de l'entité\n",
    "                            else:\n",
    "                                res[ent.text] += dict_val[dep_ent(max_span_ent,doc)] \n",
    "            else: #si l'entité n'est pas exactement coréférencée par neuralcoref\n",
    "                flag=0\n",
    "                for cluster in clusters:\n",
    "                    if is_in_cluster(ent,cluster) and ent.label_ != 'NORP': \n",
    "                        flag=1\n",
    "                        for span in cluster.mentions:\n",
    "                            #print(span)\n",
    "                            if span.ents != [] :\n",
    "                                if ent.has_vector:\n",
    "                                    max_sim=0\n",
    "                                    max_span_ent=span.ents[0]\n",
    "                                    for span_ent in span.ents:\n",
    "                                        if span_ent.has_vector:\n",
    "                                            if ent.similarity(span_ent) > max_sim:\n",
    "                                                max_span_ent=span_ent\n",
    "                                                max_sim=ent.similarity(span_ent)\n",
    "                                    ent_treated[max_span_ent.text]=1\n",
    "                            else:\n",
    "                                max_span_ent=span #on peut peut etre supprimer cette branche else et...\n",
    "                            if ent.text not in res.keys():\n",
    "                                res[ent.text]=dict_val[dep_ent(max_span_ent,doc)] #...replacer max_span_ent par span ici\n",
    "                            else:\n",
    "                                res[ent.text] += dict_val[dep_ent(max_span_ent,doc)]  #et ici\n",
    "                if flag==0: #si l'entité n'est vraiment dans aucun cluster   \n",
    "                    if ent.text not in res.keys():\n",
    "                        res[ent.text]=dict_val[dep_ent(ent,doc)]\n",
    "                    else:\n",
    "                        res[ent.text]+=dict_val[dep_ent(ent,doc)] \n",
    "    return res\n",
    "\n",
    "\n",
    "def freq_dict(L):\n",
    "    n = len(L)\n",
    "    L_unique = list(set(L))\n",
    "    d = {}\n",
    "    for a in L_unique:\n",
    "        for b in L:\n",
    "            if a == b:\n",
    "                if a in d.keys():\n",
    "                    d[a] += 1/n\n",
    "                else:\n",
    "                    d[a] = 1/n\n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sim(doc,score_doc,k):\n",
    "    res=0\n",
    "    freq_doc=freq_dict([ent.text for ent in doc.ents])\n",
    "    score_image=liste_scores[k]\n",
    "    freq_image=liste_freq[k]\n",
    "    for i in score_doc.keys():\n",
    "        if i in score_image.keys():\n",
    "            res += (score_doc[i]+score_image[i])/2 #- abs(freq_doc[i]-freq_image[i])/2\n",
    "    return res\n",
    "\n",
    "def score_sim1(doc,score_doc,k):\n",
    "    res=0\n",
    "    freq_doc=freq_dict([ent.text for ent in doc.ents])\n",
    "    score_image=liste_scores[k]\n",
    "    freq_image=liste_freq[k]\n",
    "    for i in score_doc.keys():\n",
    "        if i in score_image.keys():\n",
    "            res += (score_doc[i]+score_image[i])/2 #- abs(freq_doc[i]-freq_image[i])/2\n",
    "    return res\n",
    "\n",
    "def related_descr(doc):\n",
    "    index_list=[]\n",
    "    for k in range(len(liste_scores)):\n",
    "        for ent in doc.ents:\n",
    "            if ent.text in liste_scores[k].keys():\n",
    "                index_list.append(k)\n",
    "                break\n",
    "    return index_list\n",
    "                \n",
    "def best_image(doc):\n",
    "    score_doc=scores_doc_coref2(doc)\n",
    "    best_score=0\n",
    "    best_descr=0\n",
    "    for k in related_descr(doc):\n",
    "        if score_sim(doc,score_doc,k) > best_score:\n",
    "            best_score=score_sim(doc,score_doc,k)\n",
    "            best_descr=k\n",
    "    return best_descr #,df.description[best_descr],best_score\n",
    "\n",
    "def best_image1(doc):\n",
    "    best_score=0\n",
    "    best_descr=0\n",
    "    score_doc=scores_doc_coref1(doc)\n",
    "    for k in related_descr(doc):\n",
    "        if score_sim1(doc,score_doc,k) > best_score:\n",
    "            best_score=score_sim1(doc,score_doc,k)\n",
    "            best_descr=k\n",
    "    return best_descr #,df.description[best_descr],best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                        | 0/13775 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|███████▉                                                                  | 1471/13775 [00:00<00:00, 14578.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████████████▍                                                         | 3054/13775 [00:00<00:00, 14894.20it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████████████████                                                  | 4480/13775 [00:00<00:00, 14656.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████████████████████▍                                         | 6040/13775 [00:00<00:00, 14888.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████████████████████████▌                                 | 7559/13775 [00:00<00:00, 14937.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|████████████████████████████████████████████████▋                         | 9063/13775 [00:00<00:00, 14928.07it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|████████████████████████████████████████████████████████▎                | 10628/13775 [00:00<00:00, 15097.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|███████████████████████████████████████████████████████████████▋         | 12020/13775 [00:00<00:00, 14498.33it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|███████████████████████████████████████████████████████████████████████▊ | 13557/13775 [00:00<00:00, 14710.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 13775/13775 [00:00<00:00, 14683.48it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp(df1.description[0])\n",
    "best_image1(doc)\n",
    "#print(df.description[28], '\\n',liste_scores[28], scores_doc_coref2(doc),'\\n', df.description[11753],'\\n',liste_scores[11753])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▊                                                                                 | 1/100 [00:04<06:41,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|█▋                                                                                | 2/100 [00:06<05:43,  3.51s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|██▍                                                                               | 3/100 [00:08<05:14,  3.25s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|███▎                                                                              | 4/100 [00:15<06:51,  4.29s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|████                                                                              | 5/100 [00:18<06:01,  3.80s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|████▉                                                                             | 6/100 [00:21<05:51,  3.74s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|█████▋                                                                            | 7/100 [00:26<06:07,  3.95s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██████▌                                                                           | 8/100 [00:33<07:39,  5.00s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███████▍                                                                          | 9/100 [00:37<06:46,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|████████                                                                         | 10/100 [00:44<07:58,  5.31s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████████▉                                                                        | 11/100 [00:49<07:54,  5.33s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█████████▋                                                                       | 12/100 [00:54<07:41,  5.25s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|██████████▌                                                                      | 13/100 [00:56<06:10,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|███████████▎                                                                     | 14/100 [01:00<05:58,  4.17s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|████████████▏                                                                    | 15/100 [01:02<04:47,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|████████████▉                                                                    | 16/100 [01:04<04:22,  3.12s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█████████████▊                                                                   | 17/100 [01:09<04:49,  3.49s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████████████▌                                                                  | 18/100 [01:13<05:13,  3.82s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████████████▍                                                                 | 19/100 [01:18<05:28,  4.05s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████████████▏                                                                | 20/100 [01:22<05:25,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|█████████████████                                                                | 21/100 [01:26<05:18,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████████████▊                                                               | 22/100 [01:30<05:15,  4.05s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██████████████████▋                                                              | 23/100 [01:34<05:10,  4.04s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|███████████████████▍                                                             | 24/100 [01:38<05:03,  3.99s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|████████████████████▎                                                            | 25/100 [01:42<05:00,  4.00s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|█████████████████████                                                            | 26/100 [01:46<04:50,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|█████████████████████▊                                                           | 27/100 [01:49<04:31,  3.71s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████████████████▋                                                          | 28/100 [01:53<04:47,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████████████████▍                                                         | 29/100 [01:56<04:17,  3.62s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████████████████████▎                                                        | 30/100 [01:58<03:34,  3.07s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████████████████                                                        | 31/100 [02:02<03:57,  3.44s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|█████████████████████████▉                                                       | 32/100 [02:06<04:08,  3.65s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|██████████████████████████▋                                                      | 33/100 [02:09<03:45,  3.36s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███████████████████████████▌                                                     | 34/100 [02:13<03:45,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|████████████████████████████▎                                                    | 35/100 [02:15<03:23,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████████████████████▏                                                   | 36/100 [02:18<03:07,  2.93s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|█████████████████████████████▉                                                   | 37/100 [02:21<03:10,  3.02s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████████████████████▊                                                  | 38/100 [02:32<05:33,  5.39s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███████████████████████████████▌                                                 | 39/100 [02:34<04:36,  4.53s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████████████████████████████████▍                                                | 40/100 [02:37<04:08,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|█████████████████████████████████▏                                               | 41/100 [02:41<03:49,  3.89s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|██████████████████████████████████                                               | 42/100 [02:44<03:38,  3.77s/it]"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'max_span_ent' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-469051a4445b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mbest_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-156-aba5aa38b21e>\u001b[0m in \u001b[0;36mbest_image\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbest_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mscore_doc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscores_doc_coref2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mbest_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mbest_descr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-131-0cd921225557>\u001b[0m in \u001b[0;36mscores_doc_coref2\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    131\u001b[0m                                 \u001b[0mmax_span_ent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m                                 \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdep_ent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_span_ent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m#on affecte le score de l'entité\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                                 \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdict_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdep_ent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_span_ent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'max_span_ent' referenced before assignment"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for k in tqdm(range(100)):\n",
    "    if best_image(nlp(df.description[k]))==k:\n",
    "        count+=1\n",
    "    else:\n",
    "        print(k)\n",
    "print(str(count)+'succès sur les 100 premières images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
