{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (3,21,22,33,35,39,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5096: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82608, 54)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neuralcoref\n",
    "\n",
    "data_raw = pd.read_csv('pictures_sample.csv')\n",
    "data_english = data_raw[data_raw.Language == 'EN']\n",
    "def author_del(text):\n",
    "    text_s = text\n",
    "    while(text_s[-1] != '\\n' and len(text_s) > 1): \n",
    "        text_s = text_s[:-1]\n",
    "    if(len(text_s) <= 1):\n",
    "        return(text)\n",
    "    return(text_s[:-1])\n",
    "data_english.Description = data_english.Description.apply(author_del)          \n",
    "data_english.drop_duplicates(subset='Description', keep='first', inplace=True)\n",
    "ind = list(data_english.index)\n",
    "\n",
    "print(data_english.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_load = pd.read_pickle(\"ent.csv\")\n",
    "M = ent_load.to_numpy()\n",
    "M[M == None] = 0\n",
    "\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        if M[i,j] == 0:\n",
    "            M[i,j] = []\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        if M[i,j] != []:\n",
    "            M[i,j] = [M[i,j]]\n",
    "            \n",
    "M = list(M.sum(axis = 1))\n",
    "scoring = pd.read_csv('scoring.csv', delimiter = \";\")\n",
    "dict_val = {}\n",
    "\n",
    "for i in range(48):\n",
    "    dict_val[scoring['function'][i]] = scoring['score_norm'][i]\n",
    "    \n",
    "def dep_to_val(l):\n",
    "    L = []\n",
    "    for i in range(len(l)):\n",
    "        if l[i] in dict_val.keys():\n",
    "            L.append(dict_val[l[i]])\n",
    "        else:\n",
    "            L.append(0)\n",
    "    return(max(L))\n",
    "\n",
    "def final(l):\n",
    "    L = [] \n",
    "    for i in l:\n",
    "        ll = [] \n",
    "        for ii in i:\n",
    "            ll.append((ii[0], dep_to_val(ii[1])))\n",
    "        L.append(ll)\n",
    "    for i in range(len(L)):\n",
    "        name = L[i][0][0]\n",
    "        score = 0\n",
    "        for ii in L[i]:\n",
    "            score += ii[1]\n",
    "        L[i] = (name, score)\n",
    "    return(L)\n",
    "\n",
    "def clean_final(l):\n",
    "    L = []\n",
    "    for i in l:\n",
    "        ll = []\n",
    "        for ii in i:\n",
    "            if ii not in ll:\n",
    "                ll.append(ii)\n",
    "        L.append(ll)\n",
    "    return(L)\n",
    "\n",
    "M = [final(m) for m in M]\n",
    "M = clean_final(M)\n",
    "M = [dict(m) for m in M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x19ab55db2b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_md\")\n",
    "neuralcoref.add_to_pipe(nlp,greedyness=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today, Donald Trump met Macron in France. Trump wants this country to be happy.\n",
      "(Today, Donald Trump, Macron, France, Trump)\n",
      "{'Donald Trump': 2.0, 'France': 1.829787234, 'Macron': 0.872340426}\n",
      "{'Today': 0.70212766, 'Donald Trump': 2.0, 'Macron': 0.872340426, 'France': 1.829787234}\n",
      "{'Today': 0.70212766, 'Donald Trump': 1.0, 'Macron': 0.872340426, 'France': 0.8297872340000001, 'Trump': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def ent_in_words(i, j, doc):\n",
    "    if(j+i-1 >= len(doc)):\n",
    "        return(\"\")\n",
    "    words = \"\"\n",
    "    for k in range(i):\n",
    "        words += (doc[j+k].text + \" \")\n",
    "    return(words[:-1])\n",
    "\n",
    "def dep_ent(ent, doc):\n",
    "    start= ent.start\n",
    "    end=ent.end\n",
    "    for k in range(start,end):\n",
    "        if doc[k].head.text not in ent.text: \n",
    "            if doc[k].dep_=='conj':   #réfléchir si vraiment optimal et si on doit le faire pour compound...\n",
    "                tok=doc[k]            \n",
    "                while tok.dep_=='conj':\n",
    "                    tok=tok.head      #réfléchir à si on se décale latéralement de 1 mot/entité à place de faire .head\n",
    "                return(tok.dep_)\n",
    "            \n",
    "            if doc[k].dep_=='compound':   #réfléchir si vraiment optimal et si on doit le faire pour compound...\n",
    "                tok=doc[k]            \n",
    "                while tok.dep_=='compound':\n",
    "                    tok=tok.head      #réfléchir à si on se décale latéralement de 1 mot/entité à place de faire .head\n",
    "                return(tok.dep_)\n",
    "            return(doc[k].dep_)\n",
    "        \n",
    "    return 0\n",
    "\n",
    "def sort_ent(doc):\n",
    "    ent_coref=[ent for ent in doc.ents if ent._.is_coref and ent_good_type(ent)]\n",
    "    ent_vanilla=[ent for ent in doc.ents if ent_good_type(ent) and not ent._.is_coref]\n",
    "    return ent_coref + ent_vanilla\n",
    "\n",
    "def scores_doc(doc):\n",
    "    res={}\n",
    "    for ent in doc.ents:\n",
    "        if ent.text not in res.keys():\n",
    "            res[ent.text]=dict_val[dep_ent(ent,doc)]\n",
    "        else:\n",
    "            res[ent.text]+=dict_val[dep_ent(ent,doc)]\n",
    "    return res\n",
    "\n",
    "def scores_doc_coref1(doc):\n",
    "    doc=nlp(doc._.coref_resolved)\n",
    "    res={}\n",
    "    for ent in doc.ents:\n",
    "        if ent.text not in res.keys():\n",
    "            res[ent.text]=dict_val[dep_ent(ent,doc)]\n",
    "        else:\n",
    "            res[ent.text]+=dict_val[dep_ent(ent,doc)]\n",
    "    return res\n",
    "\n",
    "def is_in_cluster(ent,cluster):  #détermine si une entité est dans une des mentions d'un cluster\n",
    "    for span in cluster.mentions:\n",
    "        if ent.text in span.text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def ent_good_type(ent): #filtre les entités selon leur type\n",
    "    return (ent.label_ == \"PERSON\"or ent.label_ == \"NORP\" or ent.label_ == \"ORG\" or ent.label_ == \"GPE\" or ent.label_ == \"EVENT\" or ent.label_ == \"LOC\")\n",
    "\n",
    "def scores_doc_coref2(doc):\n",
    "    clusters=doc._.coref_clusters  #liste des clusters du doc. Un cluster est de la forme Trump : [Trump, he]\n",
    "    res={} #le resultat sera un dictionnaire qui permet d'associer un score à chque entité\n",
    "    ent_treated={} #dictionnaire pour différencier les entités traitées des autres\n",
    "    ents_sorted=sort_ent(doc)  #on place les entités ayant une coref avant les autres\n",
    "    for ent in ents_sorted:   \n",
    "        ent_treated[ent.text]=0  #on initialise en affectant 0 à toutes les entités\n",
    "\n",
    "    for ent in ents_sorted:     #pour chaque entité\n",
    "        if ent_treated[ent.text]==0: #si elle n'est pas considérée comme \"traitée\"\n",
    "            if ent._.is_coref:  #si elle est coréférencée (donc si elle apparait en tant que mention pour un cluster )\n",
    "                #print(ent)\n",
    "                for cluster in clusters:  #on va chercher le cluster associé à l'entité puisqu'elle est coref\n",
    "                    if ent in cluster.mentions: # on regarde si l'entité est dans les mentions du cluster\n",
    "                        for span in cluster.mentions: #Mtnt qu'on est dans le bon cluster, on regarde pour chaque span\n",
    "                            if span.ents != [] : \n",
    "                                max_sim=ent.similarity(span.ents[0])\n",
    "                                max_span_ent=span.ents[0]\n",
    "                                for span_ent in span.ents: #on cherche l'entité dans le span la plus proche de ent\n",
    "                                    if ent.similarity(span_ent) > max_sim:\n",
    "                                        max_span_ent=span_ent\n",
    "                                        max_sim=ent.similarity(span_ent)\n",
    "                                ent_treated[max_span_ent.text]=1 #une fois qu'on l'a trouvé on la marque comme traitée\n",
    "                            else:\n",
    "                                max_span_ent=span        \n",
    "                            if ent.text not in res.keys(): \n",
    "                                res[ent.text]=dict_val[dep_ent(max_span_ent,doc)]  #on affecte le score de l'entité\n",
    "                            else:\n",
    "                                res[ent.text] += dict_val[dep_ent(max_span_ent,doc)] \n",
    "            else: #si l'entité n'est pas exactement coréférencée par neuralcoref\n",
    "                flag=0\n",
    "                for cluster in clusters:\n",
    "                    if is_in_cluster(ent,cluster) and ent.label_ != 'NORP': \n",
    "                        flag=1\n",
    "                        for span in cluster.mentions:\n",
    "                            #print(span)\n",
    "                            if span.ents != [] :\n",
    "                                max_sim=ent.similarity(span.ents[0])\n",
    "                                max_span_ent=span.ents[0]\n",
    "                                for span_ent in span.ents:\n",
    "                                    if ent.similarity(span_ent) > max_sim:\n",
    "                                        max_span_ent=span_ent\n",
    "                                        max_sim=ent.similarity(span_ent)\n",
    "                                ent_treated[max_span_ent.text]=1\n",
    "                            else:\n",
    "                                max_span_ent=span #on peut peut etre supprimer cette branche else et...\n",
    "                            if ent.text not in res.keys():\n",
    "                                res[ent.text]=dict_val[dep_ent(max_span_ent,doc)] #...replacer max_span_ent par span ici\n",
    "                            else:\n",
    "                                res[ent.text] += dict_val[dep_ent(max_span_ent,doc)]  #et ici\n",
    "                if flag==0: #si l'entité n'est vraiment dans aucun cluster   \n",
    "                    if ent.text not in res.keys():\n",
    "                        res[ent.text]=dict_val[dep_ent(ent,doc)]\n",
    "                    else:\n",
    "                        res[ent.text]+=dict_val[dep_ent(ent,doc)] \n",
    "    #print(ent_treated)\n",
    "    return res\n",
    "    \n",
    "doc=nlp('Today, Donald Trump met Macron in France. Trump wants this country to be happy.')\n",
    "#doc=nlp(data_english.Description[4])\n",
    "dep_ent(doc.ents[0],doc)\n",
    "print(doc)\n",
    "print(doc.ents)\n",
    "print(scores_doc_coref2(doc))\n",
    "print(scores_doc_coref1(doc))\n",
    "print(scores_doc(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US-British actress Sienna Miller poses after unveiling her dedicated beach locker room on the Promenade des Planches at the 45th Deauville US Film Festival in Deauville, western France, on September 11, 2019. \n",
      "\n",
      "\n",
      "(US, British, Sienna Miller, the Promenade des Planches, 45th, Deauville US Film Festival, Deauville, France, September 11, 2019)\n",
      "\n",
      "\n",
      "US 0.489361702 amod\n",
      "British 0.489361702 amod\n",
      "Sienna Miller 1.0 nsubj\n",
      "the Promenade des Planches 0.8297872340000001 pobj\n",
      "45th 0.489361702 amod\n",
      "Deauville US Film Festival 0.8297872340000001 pobj\n",
      "Deauville 0.8297872340000001 pobj\n",
      "France 0.46808510600000003 appos\n",
      "September 11, 2019 0.8297872340000001 pobj\n",
      "\n",
      "\n",
      "[US-British actress Sienna Miller: [US-British actress Sienna Miller, her]]\n",
      "\n",
      "\n",
      "Résultat méthode 2 :  {'France': 0.46808510600000003, 'British': 0.489361702, 'Deauville US Film Festival': 0.8297872340000001, 'Deauville': 0.8297872340000001, 'US': 0.872340425, 'Sienna Miller': 1.382978723}\n",
      "\n",
      "\n",
      "Résultat méthode 1 :  {'France': 0.46808510600000003, '45th': 0.489361702, 'the Promenade des Planches': 0.8297872340000001, 'Deauville US Film Festival': 0.8297872340000001, 'Deauville': 0.8297872340000001, 'September 11, 2019': 0.8297872340000001, 'US': 0.978723404, 'British': 0.978723404, 'Sienna Miller': 1.7234042550000002}\n",
      "\n",
      "\n",
      "Résultat sans neuralcoref :  {'France': 0.46808510600000003, 'US': 0.489361702, 'British': 0.489361702, '45th': 0.489361702, 'the Promenade des Planches': 0.8297872340000001, 'Deauville US Film Festival': 0.8297872340000001, 'Deauville': 0.8297872340000001, 'September 11, 2019': 0.8297872340000001, 'Sienna Miller': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#doc=nlp(\"The owners of a Tucson-area Mexican restaurant are fending off social media attacks after appearing in the VIP area at a Trump rally in Phoenix last month. The Arizona Daily Star reported that a Facebook group posted a screenshot image from the rally that showed Sammy’s Mexican Grill co-owner Betty Rivas standing behind Trump, wearing a red cowboy hat emblazoned with Latinos Love Trump. The newspaper says the post attracted more than 230 comments, almost all negative. Sammy’s co-owner Jorge Rivas said some people also posted “very ugly stuff” on social media including the restaurant’s Yelp and Google pages. On Sunday Trump tweeted support for the Rivas’ restaurant. Despite apparently watching a segment on the eatery on his favourite Fox News magazine show, Fox & Friends, the president got its location wrong. The food is GREAT at Sammy’s Mexican Grill in Phoenix, Arizona, the tweet said. Congratulations to Betty & Jorge Rivas on doing such a wonderful job. I will try hard to stop by the next time I am in Phoenix. Phoenix and Tucson are 113 miles apart. Trump’s hardline immigration policies make him unpopular with most Latino Americans. Rivas said he and his wife posted a video on Facebook defending their rights as naturalized American citizens to vote, support and meet whomever they please. Just because we are Latinos it doesn’t mean that we have to feel like every other Latino in this country, Rivas said. We are individuals and we feel that we have the constitutional right to meet and support whoever we want. Jorge Rivas said the online attacks had little to no impact on business at the restaurant which he and his wife opened in 1996.\")\n",
    "\n",
    "doc=nlp(data_english.Description[0])\n",
    "print(doc)\n",
    "print('\\n')\n",
    "print(doc.ents)\n",
    "print('\\n')\n",
    "for X in doc.ents:\n",
    "    print(X,dict_val[dep_ent(X,doc)],dep_ent(X,doc))\n",
    "print('\\n')\n",
    "\n",
    "print(doc._.coref_clusters)\n",
    "print('\\n')\n",
    "\n",
    "L=scores_doc_coref2(doc)\n",
    "print(\"Résultat méthode 2 : \", dict(sorted(L.items(), key=lambda t: t[1])))\n",
    "print('\\n')\n",
    "\n",
    "L=scores_doc_coref1(doc)\n",
    "print(\"Résultat méthode 1 : \", dict(sorted(L.items(), key=lambda t: t[1])))\n",
    "print('\\n')\n",
    "\n",
    "L=scores_doc(doc)\n",
    "print(\"Résultat sans neuralcoref : \", dict(sorted(L.items(), key=lambda t: t[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sim(doc,image):\n",
    "    res=0\n",
    "    score_doc=scores_doc_coref2(doc)\n",
    "    score_image=scores_doc_coref2(image)\n",
    "    for i in score_doc.keys():\n",
    "        if i in score_image:\n",
    "            res += (score_doc[i]+score_image[i])/2\n",
    "    return res\n",
    "\n",
    "def best_image(doc):\n",
    "    best_score=0\n",
    "    best_descr=''\n",
    "    for descr in data_english.Description:\n",
    "        if score_sim(doc,descr) > best_score:\n",
    "            best_score=score_sim\n",
    "            best_descr=descr\n",
    "    return best_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution :  832.3939981460571 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "nlp_list = []\n",
    "for desc in data_english.Description[:10000]:\n",
    "    nlp_list.append(nlp(desc))\n",
    "\n",
    "print(\"Temps d'exécution : \", time.time()-start_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution :  944.046897649765 s\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "for desc in data_english.Description[10000:20000]:\n",
    "    nlp_list.append(nlp(desc))\n",
    "print(\"Temps d'exécution : \", time.time()-start_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution :  848.4053909778595 s\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "for desc in data_english.Description[20000:30000]:\n",
    "    nlp_list.append(nlp(desc))\n",
    "print(\"Temps d'exécution : \", time.time()-start_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution :  858.3026707172394 s\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "for desc in data_english.Description[30000:40000]:\n",
    "    nlp_list.append(nlp(desc))\n",
    "print(\"Temps d'exécution : \", time.time()-start_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution :  836.3889255523682 s\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "for desc in data_english.Description[40000:50000]:\n",
    "    nlp_list.append(nlp(desc))\n",
    "print(\"Temps d'exécution : \", time.time()-start_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution :  1327.91654586792 s\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "for desc in data_english.Description[50000:65000]:\n",
    "    nlp_list.append(nlp(desc))\n",
    "print(\"Temps d'exécution : \", time.time()-start_time, \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution :  760.1543135643005 s\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "for desc in data_english.Description[65000:75000]:\n",
    "    nlp_list.append(nlp(desc))\n",
    "print(\"Temps d'exécution : \", time.time()-start_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution :  676.7457978725433 s\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "for desc in data_english.Description[75000:]:\n",
    "    nlp_list.append(nlp(desc))\n",
    "print(\"Temps d'exécution : \", time.time()-start_time, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-c172666cd2fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mscores_doc_coref2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnlp_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#final_list=[]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#for k in range(len(nlp_list)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#    descr=nlp_list[k]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-c172666cd2fd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mscores_doc_coref2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnlp_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#final_list=[]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#for k in range(len(nlp_list)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#    descr=nlp_list[k]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0f968e046e3d>\u001b[0m in \u001b[0;36mscores_doc_coref2\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#si l'entité n'est vraiment dans aucun cluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                         \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdep_ent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                         \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mdict_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdep_ent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "score_list = [scores_doc_coref2(doc) for doc in nlp_list]\n",
    "\n",
    "#final_list=[]\n",
    "#for k in range(len(nlp_list)):\n",
    "#    descr=nlp_list[k]\n",
    "#    liste_score=[]\n",
    "#    score_doc=scores_doc_coref2(descr)\n",
    "#    for ent in score_doc.keys():\n",
    "#        liste_score.append([ent,score_doc[ent]])\n",
    "#     \n",
    "#    final_list.append(list_score[k])\n",
    "\n",
    "#ent_df = pd.DataFrame(score_list, dtype = 'O')\n",
    "\n",
    "# Pour sauvegarder ce DataFrame il faut utiliser la méthode to_pickle et non pas to_csv\n",
    "\n",
    "#ent_df.to_pickle(\"ent.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_load = pd.read_pickle(\"ent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_doc(doc,doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.__version__\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
