{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (3,21,22,33,35,39,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5096: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82608, 54)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neuralcoref\n",
    "\n",
    "data_raw = pd.read_csv('pictures_sample.csv')\n",
    "data_english = data_raw[data_raw.Language == 'EN']\n",
    "def author_del(text):\n",
    "    text_s = text\n",
    "    while(text_s[-1] != '\\n' and len(text_s) > 1): \n",
    "        text_s = text_s[:-1]\n",
    "    if(len(text_s) <= 1):\n",
    "        return(text)\n",
    "    return(text_s[:-1])\n",
    "data_english.Description = data_english.Description.apply(author_del)          \n",
    "data_english.drop_duplicates(subset='Description', keep='first', inplace=True)\n",
    "ind = list(data_english.index)\n",
    "\n",
    "print(data_english.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_load = pd.read_pickle(\"ent.csv\")\n",
    "M = ent_load.to_numpy()\n",
    "M[M == None] = 0\n",
    "\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        if M[i,j] == 0:\n",
    "            M[i,j] = []\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        if M[i,j] != []:\n",
    "            M[i,j] = [M[i,j]]\n",
    "            \n",
    "M = list(M.sum(axis = 1))\n",
    "scoring = pd.read_csv('scoring.csv', delimiter = \";\")\n",
    "dict_val = {}\n",
    "\n",
    "for i in range(48):\n",
    "    dict_val[scoring['function'][i]] = scoring['score_norm'][i]\n",
    "    \n",
    "def dep_to_val(l):\n",
    "    L = []\n",
    "    for i in range(len(l)):\n",
    "        if l[i] in dict_val.keys():\n",
    "            L.append(dict_val[l[i]])\n",
    "        else:\n",
    "            L.append(0)\n",
    "    return(max(L))\n",
    "\n",
    "def final(l):\n",
    "    L = [] \n",
    "    for i in l:\n",
    "        ll = [] \n",
    "        for ii in i:\n",
    "            ll.append((ii[0], dep_to_val(ii[1])))\n",
    "        L.append(ll)\n",
    "    for i in range(len(L)):\n",
    "        name = L[i][0][0]\n",
    "        score = 0\n",
    "        for ii in L[i]:\n",
    "            score += ii[1]\n",
    "        L[i] = (name, score)\n",
    "    return(L)\n",
    "\n",
    "def clean_final(l):\n",
    "    L = []\n",
    "    for i in l:\n",
    "        ll = []\n",
    "        for ii in i:\n",
    "            if ii not in ll:\n",
    "                ll.append(ii)\n",
    "        L.append(ll)\n",
    "    return(L)\n",
    "\n",
    "M = [final(m) for m in M]\n",
    "M = clean_final(M)\n",
    "M = [dict(m) for m in M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_md\")\n",
    "neuralcoref.add_to_pipe(nlp,greedyness=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today, Donald Trump met Macron in France. Trump wants this country to be happy.\n",
      "(Today, Donald Trump, Macron, France, Trump)\n",
      "{'Donald Trump': 2.0, 'France': 1.829787234, 'Today': 0.70212766, 'Macron': 0.872340426}\n",
      "{'Today': 0.70212766, 'Donald Trump': 2.0, 'Macron': 0.872340426, 'France': 1.829787234}\n",
      "{'Today': 0.70212766, 'Donald Trump': 1.0, 'Macron': 0.872340426, 'France': 0.8297872340000001, 'Trump': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def ent_in_words(i, j, doc):\n",
    "    if(j+i-1 >= len(doc)):\n",
    "        return(\"\")\n",
    "    words = \"\"\n",
    "    for k in range(i):\n",
    "        words += (doc[j+k].text + \" \")\n",
    "    return(words[:-1])\n",
    "\n",
    "def dep_ent(ent, doc):\n",
    "    start= ent.start\n",
    "    end=ent.end\n",
    "    for k in range(start,end):\n",
    "        if doc[k].head.text not in ent.text:\n",
    "            if doc[k].dep_=='conj':   #réfléchir si vraiment optimal et si on doit le faire pour compound...\n",
    "                tok=doc[k]            \n",
    "                while tok.dep_=='conj':\n",
    "                    tok=tok.head      #réfléchir à si on se décale latéralement de 1 mot/entité à place de faire .head\n",
    "                return(tok.dep_)\n",
    "            \n",
    "            if doc[k].dep_=='compound':   #réfléchir si vraiment optimal et si on doit le faire pour compound...\n",
    "                tok=doc[k]            \n",
    "                while tok.dep_=='compound':\n",
    "                    tok=tok.head      #réfléchir à si on se décale latéralement de 1 mot/entité à place de faire .head\n",
    "                return(tok.dep_)\n",
    "            return(doc[k].dep_)\n",
    "        \n",
    "    return 0\n",
    "\n",
    "def sort_ent(doc):\n",
    "    ent_coref=[]\n",
    "    ent_vanilla=[]\n",
    "    for ent in doc.ents:\n",
    "        if ent._.is_coref:\n",
    "            ent_coref.append(ent)\n",
    "        else:\n",
    "            ent_vanilla.append(ent)\n",
    "    return ent_coref + ent_vanilla\n",
    "\n",
    "def scores_doc(doc):\n",
    "    res={}\n",
    "    for ent in doc.ents:\n",
    "        if ent.text not in res.keys():\n",
    "            res[ent.text]=dict_val[dep_ent(ent,doc)]\n",
    "        else:\n",
    "            res[ent.text]+=dict_val[dep_ent(ent,doc)]\n",
    "    return res\n",
    "\n",
    "def scores_doc_coref1(doc):\n",
    "    doc=nlp(doc._.coref_resolved)\n",
    "    res={}\n",
    "    for ent in doc.ents:\n",
    "        if ent.text not in res.keys():\n",
    "            res[ent.text]=dict_val[dep_ent(ent,doc)]\n",
    "        else:\n",
    "            res[ent.text]+=dict_val[dep_ent(ent,doc)]\n",
    "    return res\n",
    "\n",
    "def is_in_cluster(ent,cluster):\n",
    "    for span in cluster.mentions:\n",
    "        if ent.text in span.text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def scores_doc_coref2(doc):\n",
    "    clusters=doc._.coref_clusters\n",
    "    res={}\n",
    "    ent_treated={}\n",
    "    for ent in doc.ents:\n",
    "        ent_treated[ent.text]=0\n",
    "    ents_sorted=sort_ent(doc)\n",
    "    for ent in ents_sorted:\n",
    "        if ent_treated[ent.text]==0:\n",
    "            if ent._.is_coref:\n",
    "                #print(ent)\n",
    "                for cluster in clusters:\n",
    "                    if ent in cluster.mentions:\n",
    "                        for span in cluster.mentions:\n",
    "                            if span.ents != [] :\n",
    "                                max_sim=ent.similarity(span.ents[0])\n",
    "                                max_span_ent=span.ents[0]\n",
    "                                for span_ent in span.ents:\n",
    "                                    if ent.similarity(span_ent) > max_sim:\n",
    "                                        max_span_ent=span_ent\n",
    "                                        max_sim=ent.similarity(span_ent)\n",
    "                                ent_treated[max_span_ent.text]=1\n",
    "                            else:\n",
    "                                max_span_ent=span        #on peut peut etre supprimer cette branche else et...\n",
    "                            if ent.text not in res.keys():\n",
    "                                res[ent.text]=dict_val[dep_ent(max_span_ent,doc)] #...replacer max_span_ent par span ici\n",
    "                            else:\n",
    "                                res[ent.text] += dict_val[dep_ent(max_span_ent,doc)]  #et ici\n",
    "            else: \n",
    "                flag=0\n",
    "                for cluster in clusters:\n",
    "                    if is_in_cluster(ent,cluster) and ent.label_ != 'NORP' and ent.label_ != 'DATE': \n",
    "                        flag=1\n",
    "                        for span in cluster.mentions:\n",
    "                            #print(span)\n",
    "                            if span.ents != [] :\n",
    "                                max_sim=ent.similarity(span.ents[0])\n",
    "                                max_span_ent=span.ents[0]\n",
    "                                for span_ent in span.ents:\n",
    "                                    if ent.similarity(span_ent) > max_sim:\n",
    "                                        max_span_ent=span_ent\n",
    "                                        max_sim=ent.similarity(span_ent)\n",
    "                                ent_treated[max_span_ent.text]=1\n",
    "                            else:\n",
    "                                max_span_ent=span #on peut peut etre supprimer cette branche else et...\n",
    "                            if ent.text not in res.keys():\n",
    "                                res[ent.text]=dict_val[dep_ent(max_span_ent,doc)] #...replacer max_span_ent par span ici\n",
    "                            else:\n",
    "                                res[ent.text] += dict_val[dep_ent(max_span_ent,doc)]  #et ici\n",
    "                if flag==0:       \n",
    "                    if ent.text not in res.keys():\n",
    "                        res[ent.text]=dict_val[dep_ent(ent,doc)]\n",
    "                    else:\n",
    "                        res[ent.text]+=dict_val[dep_ent(ent,doc)] \n",
    "    #print(ent_treated)\n",
    "    return res\n",
    "    \n",
    "doc=nlp('Today, Donald Trump met Macron in France. Trump wants this country to be happy.')\n",
    "#doc=nlp(data_english.Description[4])\n",
    "dep_ent(doc.ents[0],doc)\n",
    "print(doc)\n",
    "print(doc.ents)\n",
    "print(scores_doc_coref2(doc))\n",
    "print(scores_doc_coref1(doc))\n",
    "print(scores_doc(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The owners of a Tucson-area Mexican restaurant are fending off social media attacks after appearing in the VIP area at a Trump rally in Phoenix last month. The Arizona Daily Star reported that a Facebook group posted a screenshot image from the rally that showed Sammy’s Mexican Grill co-owner Betty Rivas standing behind Trump, wearing a red cowboy hat emblazoned with Latinos Love Trump. The newspaper says the post attracted more than 230 comments, almost all negative. Sammy’s co-owner Jorge Rivas said some people also posted “very ugly stuff” on social media including the restaurant’s Yelp and Google pages. On Sunday Trump tweeted support for the Rivas’ restaurant. Despite apparently watching a segment on the eatery on his favourite Fox News magazine show, Fox & Friends, the president got its location wrong. The food is GREAT at Sammy’s Mexican Grill in Phoenix, Arizona, the tweet said. Congratulations to Betty & Jorge Rivas on doing such a wonderful job. I will try hard to stop by the next time I am in Phoenix. Phoenix and Tucson are 113 miles apart. Trump’s hardline immigration policies make him unpopular with most Latino Americans. Rivas said he and his wife posted a video on Facebook defending their rights as naturalized American citizens to vote, support and meet whomever they please. Just because we are Latinos it doesn’t mean that we have to feel like every other Latino in this country, Rivas said. We are individuals and we feel that we have the constitutional right to meet and support whoever we want. Jorge Rivas said the online attacks had little to no impact on business at the restaurant which he and his wife opened in 1996.\n",
      "\n",
      "\n",
      "(Tucson, Mexican, Trump, Phoenix, last month, The Arizona Daily Star, Facebook, Sammy, Mexican Grill, Betty Rivas, Trump, Latinos Love Trump, more than 230, Sammy, Jorge Rivas, Google, Sunday, Trump, Rivas, Fox News, Fox & Friends, Sammy, Phoenix, Arizona, Betty & Jorge Rivas, Phoenix, Phoenix, Tucson, 113 miles, Trump, Latino Americans, Rivas, Facebook, American, Latinos, Latino, Rivas, Jorge Rivas, 1996)\n",
      "\n",
      "\n",
      "Tucson 0.8297872340000001 pobj\n",
      "Mexican 0.489361702 amod\n",
      "Trump 0.8297872340000001 pobj\n",
      "Phoenix 0.8297872340000001 pobj\n",
      "last month 0.70212766 npadvmod\n",
      "The Arizona Daily Star 1.0 nsubj\n",
      "Facebook 1.0 nsubj\n",
      "Sammy 0.38297872299999997 poss\n",
      "Mexican Grill 1.0 nsubj\n",
      "Betty Rivas 1.0 nsubj\n",
      "Trump 0.8297872340000001 pobj\n",
      "Latinos Love Trump 0.8297872340000001 pobj\n",
      "more than 230 0.191489362 nummod\n",
      "Sammy 0.38297872299999997 poss\n",
      "Jorge Rivas 1.0 nsubj\n",
      "Google 0.063829787 conj\n",
      "Sunday 0.8297872340000001 pobj\n",
      "Trump 0.8297872340000001 pobj\n",
      "Rivas 0.38297872299999997 poss\n",
      "Fox News 0.8297872340000001 pobj\n",
      "Fox & Friends 0.46808510600000003 appos\n",
      "Sammy 0.723404255 nmod\n",
      "Phoenix 0.8297872340000001 pobj\n",
      "Arizona 0.46808510600000003 appos\n",
      "Betty & Jorge Rivas 0.8297872340000001 pobj\n",
      "Phoenix 0.8297872340000001 pobj\n",
      "Phoenix 1.0 nsubj\n",
      "Tucson 1.0 nsubj\n",
      "113 miles 0.70212766 npadvmod\n",
      "Trump 0.723404255 nmod\n",
      "Latino Americans 0.8297872340000001 pobj\n",
      "Rivas 1.0 nsubj\n",
      "Facebook 0.8297872340000001 pobj\n",
      "American 0.489361702 amod\n",
      "Latinos 0.276595745 attr\n",
      "Latino 0.8297872340000001 pobj\n",
      "Rivas 1.0 nsubj\n",
      "Jorge Rivas 1.0 nsubj\n",
      "1996 0.8297872340000001 pobj\n",
      "\n",
      "\n",
      "[Trump: [Trump, Trump, Sunday Trump, Trump, his, the president, Trump, Trump’s, him], The Arizona Daily Star: [The Arizona Daily Star, The newspaper], Sammy: [Sammy, Sammy, Sammy], Sammy’s: [Sammy’s, Sammy’s, Sammy’s], Sammy’s Mexican Grill co-owner Betty Rivas: [Sammy’s Mexican Grill co-owner Betty Rivas, Sammy’s co-owner Jorge Rivas, the Rivas’], Phoenix: [Phoenix, Phoenix, Phoenix], Jorge Rivas: [Jorge Rivas, Rivas, he, his, Rivas, Jorge Rivas, he, his], he and his wife: [he and his wife, he and his wife], his wife: [his wife, his wife], Facebook: [Facebook, their], support and meet: [support and meet, they], we: [we, we, We, we, we, we]]\n",
      "\n",
      "\n",
      "Résultat méthode 2 :  {'Google': 0.063829787, 'more than 230': 0.191489362, 'Latinos': 0.276595745, 'Fox & Friends': 0.46808510600000003, 'Mexican': 0.489361702, 'American': 0.489361702, 'last month': 0.70212766, '113 miles': 0.70212766, 'Latinos Love Trump': 0.8297872340000001, 'Sunday': 0.8297872340000001, 'Fox News': 0.8297872340000001, 'Betty & Jorge Rivas': 0.8297872340000001, 'Latino Americans': 0.8297872340000001, 'Latino': 0.8297872340000001, '1996': 0.8297872340000001, 'Facebook': 1.212765957, 'Sammy': 1.489361701, 'Tucson': 1.829787234, 'The Arizona Daily Star': 2.0, 'Arizona': 2.0, 'Mexican Grill': 2.382978723, 'Betty Rivas': 2.382978723, 'Phoenix': 2.659574468, 'Rivas': 6.59574468, 'Trump': 7.148936169000001}\n",
      "\n",
      "\n",
      "Résultat méthode 1 :  {'Google': 0.063829787, 'more than 230': 0.191489362, 'Latinos': 0.276595745, 'Arizona': 0.46808510600000003, 'Mexican': 0.489361702, 'American': 0.489361702, 'last month': 0.70212766, '113 miles': 0.70212766, 'Latinos Love Trump': 0.8297872340000001, 'Trump Trump': 0.8297872340000001, 'Fox News': 0.8297872340000001, 'Fox & Friends': 0.8297872340000001, 'Betty & Jorge Rivas': 0.8297872340000001, 'Latino Americans': 0.8297872340000001, 'Latino': 0.8297872340000001, '1996': 0.8297872340000001, 'Tucson': 1.829787234, 'Sammy': 1.872340424, 'The Arizona Daily Star': 2.0, 'Facebook': 2.70212766, 'Mexican Grill': 2.8297872340000003, 'Betty Rivas': 2.8297872340000003, 'Phoenix': 3.489361702, 'Jorge Rivas': 4.0638297869999995, 'Trump': 4.382978723000001}\n",
      "\n",
      "\n",
      "Résultat sans neuralcoref :  {'Google': 0.063829787, 'more than 230': 0.191489362, 'Latinos': 0.276595745, 'Fox & Friends': 0.46808510600000003, 'Arizona': 0.46808510600000003, 'Mexican': 0.489361702, 'American': 0.489361702, 'last month': 0.70212766, '113 miles': 0.70212766, 'Latinos Love Trump': 0.8297872340000001, 'Sunday': 0.8297872340000001, 'Fox News': 0.8297872340000001, 'Betty & Jorge Rivas': 0.8297872340000001, 'Latino Americans': 0.8297872340000001, 'Latino': 0.8297872340000001, '1996': 0.8297872340000001, 'The Arizona Daily Star': 1.0, 'Mexican Grill': 1.0, 'Betty Rivas': 1.0, 'Sammy': 1.489361701, 'Tucson': 1.829787234, 'Facebook': 1.829787234, 'Jorge Rivas': 2.0, 'Rivas': 2.382978723, 'Trump': 3.2127659570000002, 'Phoenix': 3.489361702}\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(data_english.Description[9])\n",
    "\n",
    "doc=nlp(\"The owners of a Tucson-area Mexican restaurant are fending off social media attacks after appearing in the VIP area at a Trump rally in Phoenix last month. The Arizona Daily Star reported that a Facebook group posted a screenshot image from the rally that showed Sammy’s Mexican Grill co-owner Betty Rivas standing behind Trump, wearing a red cowboy hat emblazoned with Latinos Love Trump. The newspaper says the post attracted more than 230 comments, almost all negative. Sammy’s co-owner Jorge Rivas said some people also posted “very ugly stuff” on social media including the restaurant’s Yelp and Google pages. On Sunday Trump tweeted support for the Rivas’ restaurant. Despite apparently watching a segment on the eatery on his favourite Fox News magazine show, Fox & Friends, the president got its location wrong. The food is GREAT at Sammy’s Mexican Grill in Phoenix, Arizona, the tweet said. Congratulations to Betty & Jorge Rivas on doing such a wonderful job. I will try hard to stop by the next time I am in Phoenix. Phoenix and Tucson are 113 miles apart. Trump’s hardline immigration policies make him unpopular with most Latino Americans. Rivas said he and his wife posted a video on Facebook defending their rights as naturalized American citizens to vote, support and meet whomever they please. Just because we are Latinos it doesn’t mean that we have to feel like every other Latino in this country, Rivas said. We are individuals and we feel that we have the constitutional right to meet and support whoever we want. Jorge Rivas said the online attacks had little to no impact on business at the restaurant which he and his wife opened in 1996.\")\n",
    "print(doc)\n",
    "print('\\n')\n",
    "print(doc.ents)\n",
    "print('\\n')\n",
    "for X in doc.ents:\n",
    "    print(X,dict_val[dep_ent(X,doc)],dep_ent(X,doc))\n",
    "print('\\n')\n",
    "\n",
    "print(doc._.coref_clusters)\n",
    "print('\\n')\n",
    "\n",
    "L=scores_doc_coref2(doc)\n",
    "print(\"Résultat méthode 2 : \", dict(sorted(L.items(), key=lambda t: t[1])))\n",
    "print('\\n')\n",
    "\n",
    "L=scores_doc_coref1(doc)\n",
    "print(\"Résultat méthode 1 : \", dict(sorted(L.items(), key=lambda t: t[1])))\n",
    "print('\\n')\n",
    "\n",
    "L=scores_doc(doc)\n",
    "print(\"Résultat sans neuralcoref : \", dict(sorted(L.items(), key=lambda t: t[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
