{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procédure pour créer le fichier .csv contenant les entités nommées ainsi que leurs fonctions grammaticales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import spacy\n",
    "import en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load() # On charge une fonction de nlp anglaise de spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dimit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (3,21,22,33,35,39,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_raw = pd.read_csv('pictures_sample.csv') # On charge la base de données dans un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dimit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\generic.py:5096: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "c:\\users\\dimit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(82608, 54)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_english = data_raw[data_raw.Language == 'EN'] # On ne conserve que les images dont la langue renseignée est l'anglais\n",
    "\n",
    "# On constate que certaines descriptions sont identiques à l'auteur près, si on affiche les deux premières descriptions on a:\n",
    "\n",
    "print(data_english.Description[0])\n",
    "print(data_english.Description[1])\n",
    "\n",
    "# On va donc créer une fonction qui supprime le nom de l'auteur (si il y en a un) dans chaque description\n",
    "\n",
    "def author_del(text):\n",
    "    text_s = text\n",
    "    while(text_s[-1] != '\\n' and len(text_s) > 1): # On veut supprimer la dernière ligne sautée, sans pour autant supprimer\n",
    "                                                   # tout le texte\n",
    "        text_s = text_s[:-1]\n",
    "    if(len(text_s) <= 1):\n",
    "        return(text)\n",
    "    return(text_s[:-1])\n",
    "\n",
    "data_english.Description = data_english.Description.apply(author_del)          # On applique la fonction précédente au DF\n",
    "data_english.drop_duplicates(subset='Description', keep='first', inplace=True) # On supprime les doublons\n",
    "ind = list(data_english.index)\n",
    "\n",
    "# On a divisé par deux le nombre d'image\n",
    "\n",
    "print(data_english.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copier-coller la case suivante afin d'obtenir la base de donnée \"nettoyée\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('pictures_sample.csv')\n",
    "data_english = data_raw[data_raw.Language == 'EN']\n",
    "def author_del(text):\n",
    "    text_s = text\n",
    "    while(text_s[-1] != '\\n' and len(text_s) > 1): \n",
    "        text_s = text_s[:-1]\n",
    "    if(len(text_s) <= 1):\n",
    "        return(text)\n",
    "    return(text_s[:-1])\n",
    "data_english.Description = data_english.Description.apply(author_del)          \n",
    "data_english.drop_duplicates(subset='Description', keep='first', inplace=True)\n",
    "ind = list(data_english.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée une liste dans laquelle nous allons stocker l'application de la fonction nlp à toutes nos descriptions\n",
    "\n",
    "nlp_list = []\n",
    "\n",
    "for desc in data_english.Description:\n",
    "    nlp_list.append(nlp(desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit une fonction pour retrouver une entité nommée dans une liste de mots ordonnée\n",
    "\n",
    "# i correspond au nombre de mots de l'entité nommée, et j au mot auquel on s'intéresse dans la description\n",
    "\n",
    "def ent_in_words(i, j, desc):\n",
    "    if(j+i-1 >= len(desc)):\n",
    "        return(\"\")\n",
    "    words = \"\"\n",
    "    for k in range(i):\n",
    "        words += (desc[j+k].text + \" \")\n",
    "    return(words[:-1]) # On renvoit les i prochains mots à partir du j-ième mot\n",
    "\n",
    "# Comme la fonction précédente sauf que l'on renvoit les fonctions grammaticales\n",
    "\n",
    "def dep_in_ent(i, j, desc):\n",
    "    l = []\n",
    "    for k in range(i):\n",
    "        l.append(desc[j+k].dep_)\n",
    "    return(l)\n",
    "\n",
    "# On supprime les listes vides contenues dans l\n",
    "\n",
    "def clear_blanks(l):\n",
    "    L = []\n",
    "    for i in range(len(l)):\n",
    "        if(len(l[i]) != 0):\n",
    "            L.append(l[i])\n",
    "    return(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée une liste qui servira à stocker les entités nommées ainsi que leurs fonctions grammaticales pour chaque description\n",
    "\n",
    "final_list = []\n",
    "\n",
    "# On la remplie\n",
    "\n",
    "for k in range(len(nlp_list)): \n",
    "    desc = nlp_list[k]\n",
    "    \n",
    "    # Pour chaque description, on crée une liste de liste de tuple:\n",
    "    # - chaque liste dans la liste correspondra aux différentes occurences de chaque entités nommées\n",
    "    # - chaque tuple correspondra au nom de l'entité nommée ainsi que sa(ses) fonctions grammaticales correspond à \n",
    "    # cette occurence\n",
    "    \n",
    "    ents_desc = [[(ent.text, dep_in_ent(len(ent.text.split()), i, desc)) for i in range(len(desc))\n",
    "                                                              if (ent.text == ent_in_words(len(ent.text.split()), i, desc)) \n",
    "                                                             and (ent.label_ == \"PERSON\" \n",
    "                                                               or ent.label_ == \"NORP\" \n",
    "                                                               or ent.label_ == \"ORG\"\n",
    "                                                               or ent.label_ == \"GPE\" \n",
    "                                                               or ent.label_ == \"EVENT\"\n",
    "                                                               or ent.label_ == \"LOC\")] for ent in nlp_list[k].ents]\n",
    "    \n",
    "    # On supprime les listes vides correspondant aux entités nommées dites inintéressantes\n",
    "    \n",
    "    ents_desc = clear_blanks(ents_desc)\n",
    "    final_list.append(ents_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sauvegarde la liste crée sous forme de DataFrame ayant un format particulier (car ses colonnes seront des objets, en\n",
    "# particulier des listes)\n",
    "\n",
    "ent_df = pd.DataFrame(final_list, dtype = 'O')\n",
    "\n",
    "# Pour sauvegarder ce DataFrame il faut donc utilisé la méthode to_pickle et non pas to_csv\n",
    "\n",
    "ent_df.to_pickle(\"ent.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procédure pour lire le fichier .csv contenant les entités nommées ainsi que leurs fonctions grammaticales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On lit le fichier avec read_pickle étant donné qu'il contient des objets (en particulier des listes)\n",
    "\n",
    "ent_load = pd.read_pickle(\"ent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On transforme le DataFrame en matrice afin de retrouver son format original\n",
    "\n",
    "M = ent_load.to_numpy()\n",
    "M[M == None] = 0 # On remplace les colonnes \"None\" par 0 (on ne peut pas les remplacer par des listes vides)\n",
    "\n",
    "# On les remplacent donc après par des listes vides\n",
    "\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        if M[i,j] == 0:\n",
    "            M[i,j] = []\n",
    "\n",
    "# On remplace chaque élément par lui-même dans une liste afin de pouvoir concaténer les colonnes dans une grosse liste via la\n",
    "# méthode sum, mais pour ne pas avoir de listes vides on ne remplacent pas les listes vides par [[]].\n",
    "\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        if M[i,j] != []:\n",
    "            M[i,j] = [M[i,j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ainsi en sommant les colonnes, on obtient l'objet sous sa forme originale\n",
    "\n",
    "M = list(M.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe le fichier des scores que l'on transforme en dictionnaire\n",
    "\n",
    "scoring = pd.read_csv('scoring.csv', delimiter = \";\")\n",
    "dict_val = {}\n",
    "for i in range(48):\n",
    "    dict_val[scoring['function'][i]] = scoring['score_norm'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée une fonction qui à une liste de fonctions grammaticales renvoit le maximum des scores associés d'après le .csv\n",
    "\n",
    "def dep_to_val(l):\n",
    "    L = []\n",
    "    for i in range(len(l)):\n",
    "        if l[i] in dict_val.keys():\n",
    "            L.append(dict_val[l[i]])\n",
    "        else:\n",
    "            # Si la fonction grammaticale n'est pas dans le fichier, on associe un score de 0\n",
    "            L.append(0)\n",
    "    return(max(L))\n",
    "\n",
    "# On définit une fonction qui retournera l'ensembles des entités et leurs scores associés grâce à la fonction prédécente pour\n",
    "# une description\n",
    "\n",
    "def final(l):\n",
    "    L = [] # Liste finale\n",
    "    \n",
    "    # Pour entité de la description\n",
    "    \n",
    "    for i in l:\n",
    "        ll = [] # On crée sa liste finale associée\n",
    "        \n",
    "        # On l'a rempli par son nom et sa valeur associée\n",
    "        \n",
    "        for ii in i:\n",
    "            ll.append((ii[0], dep_to_val(ii[1])))\n",
    "        \n",
    "        # On rajoute cette liste dans la liste finale\n",
    "        \n",
    "        L.append(ll)\n",
    "        \n",
    "    # Pour chaque entité de la liste finale, on somme les scores de l'entité\n",
    "    \n",
    "    for i in range(len(L)):\n",
    "        name = L[i][0][0]\n",
    "        score = 0\n",
    "        for ii in L[i]:\n",
    "            score += ii[1]\n",
    "        L[i] = (name, score)\n",
    "    return(L)\n",
    "\n",
    "# On crée une fonction qui ne gardera qu'une seule fois plusieurs entités (leur score prennant déjà en compte le nombre\n",
    "# d'apparition)\n",
    "\n",
    "def clean_final(l):\n",
    "    L = []\n",
    "    for i in l:\n",
    "        ll = []\n",
    "        for ii in i:\n",
    "            if ii not in ll:\n",
    "                ll.append(ii)\n",
    "        L.append(ll)\n",
    "    return(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique la fonction final à chaque description de notre liste\n",
    "\n",
    "M = [final(m) for m in M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On nettoie les entités en double \n",
    "\n",
    "M = clean_final(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met tout ça sous forme de dictionnaire pour en faciliter l'utilisation\n",
    "\n",
    "M = [dict(m) for m in M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'US': 0.7659574469999999, 'Sienna Miller': 1.0, 'Deauville US Film Festival': 0.872340426, 'Deauville': 0.8936170219999999, 'France': 0.5106382979999999}, {'Shatin': 0.021276596000000002, 'Hong Kong': 1.744680852}, {'AFP': 1.0, 'Christof Stache': 0.872340426}, {'Palestinian': 0.531914894, 'Jordan Valley': 0.936170213, 'Jiftlik': 0.872340426, 'Israeli': 1.2765957449999998, 'West Bank': 1.297872341, 'Benjamin Netanyahu': 0.425531915, 'Netanyahu': 1.425531915, 'Palestinians': 0.872340426, 'Arab': 0.531914894, 'the United Nations': 0.46808510600000003, 'the European Union': 0.46808510600000003}, {'Turkish': 0.531914894, 'Ruhsar Pekcan': 1.0, 'US': 0.765957447, 'Wilbur Ross': 0.106382979, 'Ankara': 0.872340426}, {'Mini Electric': 0.021276596000000002, 'Mini': 0.8936170219999999, 'the IAA Car Show': 0.872340426, 'Frankfurt': 1.297872341, 'biennial International Auto Show': 1.0}, {'BMW': 1.638297873, 'German': 0.531914894, 'Frankfurt': 1.297872341, 'Germany': 0.5106382979999999, 'the International Auto Show': 0.872340426, 'biennial International Auto Show': 1.0}, {'Lebanese': 1.063829788, 'Shiite': 1.063829788, 'Ashura': 1.744680852, 'Nabatieh': 0.872340426, 'Shiite Muslims': 0.872340426, 'Imam Hussein': 0.872340426, 'Prophet Mohammed': 0.872340426}, {'Canadian': 0.531914894, 'Margaret Atwood': 1.0, 'London': 0.872340426}, {'Lebanese': 1.063829788, 'Shiite': 1.063829788, 'Ashura': 1.744680852, 'Nabatieh': 0.872340426, 'Shiite Muslims': 0.872340426, 'Imam Hussein': 0.872340426, 'Prophet Mohammed': 0.872340426}]\n"
     ]
    }
   ],
   "source": [
    "# Affichage des dictionnaires des dix premières descriptions\n",
    "\n",
    "print(M[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copier-coller la case suivante afin de pouvoir directement obtenir M dans un autre notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_load = pd.read_pickle(\"ent.csv\")\n",
    "M = ent_load.to_numpy()\n",
    "M[M == None] = 0\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        if M[i,j] == 0:\n",
    "            M[i,j] = []\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        if M[i,j] != []:\n",
    "            M[i,j] = [M[i,j]]\n",
    "M = list(M.sum(axis = 1))\n",
    "scoring = pd.read_csv('scoring.csv', delimiter = \";\")\n",
    "dict_val = {}\n",
    "for i in range(48):\n",
    "    dict_val[scoring['function'][i]] = scoring['score_norm'][i]\n",
    "def dep_to_val(l):\n",
    "    L = []\n",
    "    for i in range(len(l)):\n",
    "        if l[i] in dict_val.keys():\n",
    "            L.append(dict_val[l[i]])\n",
    "        else:\n",
    "            L.append(0)\n",
    "    return(max(L))\n",
    "def final(l):\n",
    "    L = [] \n",
    "    for i in l:\n",
    "        ll = [] \n",
    "        for ii in i:\n",
    "            ll.append((ii[0], dep_to_val(ii[1])))\n",
    "        L.append(ll)\n",
    "    for i in range(len(L)):\n",
    "        name = L[i][0][0]\n",
    "        score = 0\n",
    "        for ii in L[i]:\n",
    "            score += ii[1]\n",
    "        L[i] = (name, score)\n",
    "    return(L)\n",
    "def clean_final(l):\n",
    "    L = []\n",
    "    for i in l:\n",
    "        ll = []\n",
    "        for ii in i:\n",
    "            if ii not in ll:\n",
    "                ll.append(ii)\n",
    "        L.append(ll)\n",
    "    return(L)\n",
    "M = [final(m) for m in M]\n",
    "M = clean_final(M)\n",
    "M = [dict(m) for m in M]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
