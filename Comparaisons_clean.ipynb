{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (10,15,17,23,24,29,30,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1b186d944a8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neuralcoref\n",
    "import ast\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "scoring = pd.read_csv('scoring.csv', delimiter = \";\")\n",
    "dict_val = {}\n",
    "\n",
    "for i in range(48):\n",
    "    dict_val[scoring['function'][i]] = scoring['score_norm'][i]\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_md\")\n",
    "neuralcoref.add_to_pipe(nlp,greedyness=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_del_photos(text):\n",
    "    text_s = text\n",
    "    while(text_s[-7:] != ' / AFP ' and len(text_s) > 1): \n",
    "        text_s = text_s[:-1]\n",
    "    if(len(text_s) <= 1):\n",
    "        return(text)\n",
    "    return(text_s[:-7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THINKPAD\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (11,16,24,25,30,31,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "def transfo_event(ev):\n",
    "    res=ev\n",
    "    return res[1:-1].replace(\"'\",\"\").split(', ')\n",
    "\n",
    "df_depeche=pd.read_csv('depeches_clean.csv',index_col='Unnamed: 0')\n",
    "df_depeche.event=df_depeche.event.apply(transfo_event)\n",
    "\n",
    "df2=pd.read_csv('photos_clean.csv',index_col='Unnamed: 0')\n",
    "df2.event=df2.event.apply(transfo_event)\n",
    "\n",
    "df2.caption = df2.caption.apply(author_del_photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scores(csv):\n",
    "    file=pd.read_csv(csv)\n",
    "    res=[]\n",
    "    for string in file.scores:\n",
    "        res.append(ast.literal_eval(string))\n",
    "    return res\n",
    "\n",
    "\n",
    "liste_score2=read_scores(\"event_score_2.csv\")\n",
    "liste_score4=read_scores(\"event_score_4.csv\")\n",
    "\n",
    "liste_score2_d=read_scores(\"depeche_score_2.csv\")\n",
    "liste_score4_d=read_scores(\"depeche_score_4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarité en normalisant par le max.  Pour best5_image_test, j correspond a l'indice de l'image dont on veut tester si elle est dans les 5 meilleures images retournées par l'algo. liste_scores_d est un param optionnel qui s'il est non vide permet de spécifier que j est l'indice d'une dépêche plutot que d'une image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sim(score_doc,liste_scores,k):\n",
    "    res=0\n",
    "    score_image=liste_scores[k]\n",
    "    if len(score_image.values())==0 or len(score_doc.values())==0:\n",
    "        return 0\n",
    "    max_doc=max(score_doc.values())\n",
    "    max_im=max(score_image.values())\n",
    "    for i in score_doc.keys():\n",
    "        if i in score_image.keys():\n",
    "            if max_doc==0 or max_im==0:\n",
    "                return 0\n",
    "            res += score_doc[i]/max_doc +score_image[i]/max_im\n",
    "    return res\n",
    "\n",
    "def best5_image_test(j,liste_scores,liste_scores_d=None):\n",
    "    rel_descr=range(len(liste_scores))\n",
    "    \n",
    "    if liste_scores_d == None:\n",
    "        doc_score=liste_scores[j]\n",
    "    else:\n",
    "        doc_score=liste_scores_d[j]\n",
    "        \n",
    "    best_scores=[(i,score_sim(doc_score,liste_scores,i)) for i in rel_descr[:5]]\n",
    "    best_scores.sort(key = lambda x : x[1])\n",
    "    for k in rel_descr[5:]:\n",
    "        score_simil=score_sim(doc_score,liste_scores,k)\n",
    "        if score_simil > best_scores[0][1]:\n",
    "            best_scores.pop(0)\n",
    "            best_scores.append((k,score_sim(doc_score,liste_scores,k)))\n",
    "            best_scores.sort(key = lambda x : x[1])\n",
    "    return best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on regarde ce que ça donne en normalisant par la somme des scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sim_bis(score_doc,liste_scores,k):\n",
    "    res=0\n",
    "    score_image=liste_scores[k]\n",
    "    if len(score_image.values())==0 or len(score_doc.values())==0:\n",
    "        return 0\n",
    "    max_doc=max(score_doc.values())\n",
    "    max_im=max(score_image.values())\n",
    "    for i in score_doc.keys():\n",
    "        if i in score_image.keys():\n",
    "            if max_doc==0 or max_im==0:\n",
    "                return 0\n",
    "            res += score_doc[i]/sum(score_doc.values()) +score_image[i]/sum(score_image.values())\n",
    "    return res\n",
    "\n",
    "def best5_image_bis_test(j,liste_scores,liste_scores_d=None):\n",
    "    rel_descr=range(len(liste_scores))\n",
    "    \n",
    "    if liste_scores_d == None:\n",
    "        doc_score=liste_scores[j]\n",
    "    else:\n",
    "        doc_score=liste_scores_d[j]\n",
    "    \n",
    "    best_scores=[(i,score_sim_bis(doc_score,liste_scores,i)) for i in rel_descr[:5]]\n",
    "    best_scores.sort(key = lambda x : x[1])\n",
    "    for k in rel_descr[5:]:\n",
    "        score_simil=score_sim_bis(doc_score,liste_scores,k)\n",
    "        if score_simil > best_scores[0][1]:\n",
    "            best_scores.pop(0)\n",
    "            best_scores.append((k,score_sim_bis(doc_score,liste_scores,k)))\n",
    "            best_scores.sort(key = lambda x : x[1])\n",
    "    return best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1er test : on regarde si la meilleure caption selon l'algo est la caption elle meme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d99efd420d4a8aa472fb5465a1cc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[80, 65, 91, 91, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.reset_index(drop=True,inplace=True)\n",
    "df_depeche.reset_index(drop=True,inplace=True)\n",
    "\n",
    "L=[]\n",
    "for k in range(100):\n",
    "    l=np.random.randint(0,len(df2))\n",
    "    while l in L:\n",
    "        l=np.random.randint(0,len(df2))\n",
    "    L.append(l)\n",
    "    \n",
    "count=[0 for k in range(4)]\n",
    "for i in tqdm_notebook(L):\n",
    "\n",
    "    a1=best5_image_test(i,liste_score2)\n",
    "    a2=best5_image_test(i,liste_score4)\n",
    "    \n",
    "    a3=best5_image_bis_test(i,liste_score2)\n",
    "    \n",
    "    a4=best5_image_bis_test(i,liste_score4)\n",
    "    \n",
    "    for tup in a1:\n",
    "        if tup[0]==i:\n",
    "            count[0]+=1\n",
    "            break\n",
    "    for tup in a2:\n",
    "        if tup[0]==i:\n",
    "            count[1]+=1\n",
    "            break\n",
    "    for tup in a3:\n",
    "        if tup[0]==i:\n",
    "            count[2]+=1\n",
    "            break\n",
    "    for tup in a3:\n",
    "        if tup[0]==i:\n",
    "            count[3]+=1\n",
    "            break\n",
    "    \n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que c'est bcp plus performant en normalisant par la somme des scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2eme test : on regarde si dans les 5 photos retournées il  y a l'evenement de la depeche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ev_match(ev_list1,ev_list2):\n",
    "    for ev in ev_list1:\n",
    "        if ev in ev_list2:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbef73cb22d74dbc896f065b9c50957d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[63, 59, 64, 59, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.reset_index(drop=True,inplace=True)\n",
    "df_depeche.reset_index(drop=True,inplace=True)\n",
    "\n",
    "L=[]\n",
    "for k in range(100):\n",
    "    l=np.random.randint(0,len(df_depeche))\n",
    "    while l in L:\n",
    "        l=np.random.randint(0,len(df_depeche))\n",
    "    L.append(l)\n",
    "    \n",
    "count=[0 for k in range(8)]\n",
    "for i in tqdm_notebook(L):\n",
    "    a1=best5_image_test(i,liste_score2,liste_score2_d)\n",
    "    a2=best5_image_test(i,liste_score4,liste_score4_d)\n",
    "    \n",
    "    a3=best5_image_bis_test(i,liste_score2,liste_score2_d)\n",
    "    \n",
    "    a4=best5_image_bis_test(i,liste_score4,liste_score4_d)\n",
    "    for tup in a1:\n",
    "        if ev_match(df_depeche.event[i],df2.event[tup[0]]):\n",
    "            count[0]+=1\n",
    "            break\n",
    "    for tup in a2:\n",
    "        if ev_match(df_depeche.event[i],df2.event[tup[0]]):\n",
    "            count[1]+=1\n",
    "            break\n",
    "    for tup in a3:\n",
    "        if ev_match(df_depeche.event[i],df2.event[tup[0]]):\n",
    "            count[2]+=1\n",
    "            break\n",
    "    for tup in a4:\n",
    "        if ev_match(df_depeche.event[i],df2.event[tup[0]]):\n",
    "            count[3]+=1\n",
    "            break\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
